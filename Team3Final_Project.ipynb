{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nr0_6JmMpUK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate  # Êñ∞Â¢û\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"APIKEY\")  # set in environment for security\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# ----------------- LLM INIT -----------------\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.5, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# ----------------- SYSTEM PROMPT -----------------\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are an expert document-based assistant, helping users explore and understand the content of the uploaded PDFs.\n",
        "\n",
        "1. Always ground your answers in the provided PDF context. Cite page numbers in parentheses, e.g. (page 10).\n",
        "2. If the document does not contain enough information to answer the question, respond:\n",
        "   ‚ÄúI don‚Äôt have enough information in the document to answer that. Could you please clarify or ask a different question?‚Äù\n",
        "3. You may ask follow-up questions to better understand the user‚Äôs intent, but never introduce facts not supported by the PDF.\n",
        "4. Keep your tone friendly, concise, and professional.\n",
        "\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\".strip(),\n",
        ")\n",
        "\n",
        "# ----------------- PAGE SETUP -----------------\n",
        "st.set_page_config(\n",
        "    page_title=\"Chat with Your PDFs\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\",\n",
        ")\n",
        "\n",
        "# Custom CSS for a cleaner look\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .sidebar .sidebar-content {background-color: #f0f2f6;}\n",
        "    .css-1d391kg {background-color: white;}  /* Main panel card bg */\n",
        "    .stButton>button {background-color: #4B8BBE; color: white; border-radius: 8px;}\n",
        "    .stFileUploader>div {padding: 10px; border: 2px dashed #a2a9b7; border-radius: 8px;}\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ----------------- SIDEBAR -----------------\n",
        "st.sidebar.title(\"‚öôÔ∏è Settings\")\n",
        "# Model selection\n",
        "model_name = st.sidebar.selectbox(\n",
        "    \"Model:\",\n",
        "    options=[\"gpt-4o\", \"gpt-3.5-turbo\"],\n",
        "    index=0,\n",
        ")\n",
        "llm.model_name = model_name\n",
        "\n",
        "# Retriever parameters\n",
        "st.sidebar.subheader(\"Retriever Settings\")\n",
        "k = st.sidebar.slider(\"Top context chunks (k)\", min_value=2, max_value=12, value=6)\n",
        "fetch_k = st.sidebar.slider(\"Candidate pool (fetch_k)\", min_value=10, max_value=50, value=20)\n",
        "lambda_mult = st.sidebar.slider(\"Diversity (lambda)\", min_value=0.1, max_value=1.0, value=0.8)\n",
        "\n",
        "# ----------------- TITLE -----------------\n",
        "st.markdown(\"# üìÑüí¨ Chat with Your PDFs\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# ----------------- UPLOAD & PROCESS -----------------\n",
        "with st.expander(\"üìÇ Upload & Process PDFs\", expanded=True):\n",
        "    files = st.file_uploader(\n",
        "        \"Drag & drop PDF files here or click to browse\",\n",
        "        type=[\"pdf\"], accept_multiple_files=True\n",
        "    )\n",
        "    if files:\n",
        "        if \"vector_store\" not in st.session_state:\n",
        "            with st.spinner(\"üîç Indexing documents...\"):\n",
        "                docs = []\n",
        "                with tempfile.TemporaryDirectory() as td:\n",
        "                    for f in files:\n",
        "                        path = os.path.join(td, f.name)\n",
        "                        with open(path, \"wb\") as out:\n",
        "                            out.write(f.getbuffer())\n",
        "                        pages = PyPDFLoader(path).load()\n",
        "                        docs.extend(pages)\n",
        "\n",
        "                chunks = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=500, chunk_overlap=50\n",
        "                ).split_documents(docs)\n",
        "                embeddings = OpenAIEmbeddings()\n",
        "                st.session_state.vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "            st.success(\"‚úÖ Documents indexed! Start asking below.\")\n",
        "\n",
        "# ----------------- CHAT -----------------\n",
        "# Initialize chat history storage\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "if \"vector_store\" in st.session_state:\n",
        "    # Render past messages\n",
        "    for msg in st.session_state.messages:\n",
        "        with st.chat_message(msg[\"role\"]):\n",
        "            st.markdown(msg[\"content\"])\n",
        "\n",
        "    # Chat input\n",
        "    user_q = st.chat_input(\"Ask a question about your PDFs‚Ä¶\")\n",
        "    if user_q:\n",
        "        st.session_state.messages.append({\"role\":\"user\",\"content\":user_q})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(user_q)\n",
        "\n",
        "        retriever = st.session_state.vector_store.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\"k\": k, \"fetch_k\": fetch_k, \"lambda_mult\": lambda_mult}\n",
        "        )\n",
        "        # Âú®ËøôÈáåÊ≥®ÂÖ• promptÔºåÂÖ∂‰ªñ‰∏çÂèò\n",
        "        qa = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            chain_type=\"stuff\",\n",
        "            chain_type_kwargs={\"prompt\": prompt},\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        with st.spinner(\"ü§î Thinking‚Ä¶\"):\n",
        "            resp = qa.invoke({\"query\": user_q})\n",
        "        answer = resp[\"result\"]\n",
        "\n",
        "        st.session_state.messages.append({\"role\":\"assistant\",\"content\":answer})\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(answer)\n",
        "else:\n",
        "    st.info(\"üì• Please upload and process PDFs to start.\")\n"
      ]
    }
  ]
}