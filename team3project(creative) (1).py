# -*- coding: utf-8 -*-
"""Team3Project(creative).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u8vDsiICEEcQEZA_yjCkqLd_0FShcCEy
"""

import os
import tempfile
import time
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ----------------- CONFIG -----------------
# Load API key from env and register it for OpenAI calls
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "APIKEY")  # set in environment for security
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# ----------------- LLM INIT -----------------
# Instantiate the ChatOpenAI model (gpt-4o by default) with moderate temperature
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.9, openai_api_key=OPENAI_API_KEY)

# ----------------- SYSTEM PROMPT -----------------
# Define a PromptTemplate that instructs the model how to behave on each query
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an expert document-based assistant, helping users explore and understand the content of the uploaded PDFs.

1. Always ground your answers in the provided PDF context. Cite page numbers in parentheses, e.g. (page 10).
2. If the document does not contain enough information to answer the question, respond:
   ‚ÄúI don‚Äôt have enough information in the document to answer that. Could you please clarify or ask a different question?‚Äù or you can go ahead and ‚Äúfill in the gaps‚Äù with reasonable, creative speculation.
3. You may ask follow-up questions to better understand the user‚Äôs intent, but never introduce facts not supported by the PDF.
4. Keep your tone friendly, concise, and professional.
5. Cite actual pages when you quote from the PDF (e.g. ‚ÄúAccording to page 12‚Ä¶‚Äù), but invented facts need no citation.



Context:
{context}

Question:
{question}
""".strip(),
)

# ----------------- PAGE SETUP -----------------
# Configure Streamlit app appearance and layout
st.set_page_config(
    page_title="Chat with Your PDFs",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ----------------- CUSTOM CSS -----------------
# Inject CSS to style sidebar, buttons, and uploader
st.markdown(
    """
    <style>
    .sidebar .sidebar-content {background-color: #f0f2f6;}
    .css-1d391kg {background-color: white;}  /* Main panel card bg */
    .stButton>button {background-color: #4B8BBE; color: white; border-radius: 8px;}
    .stFileUploader>div {padding: 10px; border: 2px dashed #a2a9b7; border-radius: 8px;}
    </style>
    """,
    unsafe_allow_html=True
)

# ----------------- CREATIVE PROMPT (allows ‚Äúhallucination‚Äù) -----------------
creative_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an imaginative document assistant. You have the PDF context below but may also invent
plausible details to flesh out your answer.

1. Base your answer on the context when it‚Äôs available, but if the PDF doesn‚Äôt fully answer the
   question, go ahead and ‚Äúfill in the gaps‚Äù with reasonable speculation.
2. Cite actual pages when you quote from the PDF (e.g. ‚ÄúAccording to page 12‚Ä¶‚Äù), but made-up
   details don‚Äôt require citations.
3. Keep your tone friendly and engaging.

Context:
{context}

Question:
{question}
""".strip(),
)

# ----------------- SIDEBAR -----------------
# Title and model selector in sidebar
st.sidebar.title("‚öôÔ∏è Settings")
model_name = st.sidebar.selectbox(
    "Model:",
    options=["gpt-4o", "gpt-3.5-turbo"],
    index=0,
)
llm.model_name = model_name

allow_creative = st.sidebar.checkbox("Allow creative speculation (hallucinate)", value=False)

prompt_to_use = creative_prompt if allow_creative else prompt


# Retriever parameter sliders for MMR retrieval
st.sidebar.subheader("Retriever Settings")
k = st.sidebar.slider("Top context chunks (k)", min_value=2, max_value=12, value=6)
fetch_k = st.sidebar.slider("Candidate pool (fetch_k)", min_value=10, max_value=50, value=20)
lambda_mult = st.sidebar.slider("Diversity (lambda)", min_value=0.1, max_value=1.0, value=0.8)

# ----------------- TITLE -----------------
# Main page title and divider
st.markdown("# üìÑüí¨ Chat with Your PDFs")
st.markdown("---")

# ----------------- UPLOAD & PROCESS -----------------
# Expander section to handle PDF uploads and indexing
with st.expander("üìÇ Upload & Process PDFs", expanded=True):
    files = st.file_uploader(
        "Drag & drop PDF files here or click to browse",
        type=["pdf"], accept_multiple_files=True
    )
    if files:
        if "vector_store" not in st.session_state:
            with st.spinner("üîç Indexing documents..."):
                docs = []
                # Save uploaded PDFs to temp files and load pages
                with tempfile.TemporaryDirectory() as td:
                    for f in files:
                        path = os.path.join(td, f.name)
                        with open(path, "wb") as out:
                            out.write(f.getbuffer())
                        pages = PyPDFLoader(path).load()
                        docs.extend(pages)

                # Split pages into chunks with overlap for retrieval
                chunks = RecursiveCharacterTextSplitter(
                    chunk_size=500, chunk_overlap=50
                ).split_documents(docs)
                # Embed chunks and build FAISS vector store
                embeddings = OpenAIEmbeddings()
                st.session_state.vector_store = FAISS.from_documents(chunks, embeddings)
            st.success("‚úÖ Documents indexed! Start asking below.")

# ----------------- CHAT -----------------
# Initialize chat history if not present
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_store" in st.session_state:
    # Render previous chat messages
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Input box for new questions
    user_q = st.chat_input("Ask a question about your PDFs‚Ä¶")
    if user_q:
        # Save and display user question
        st.session_state.messages.append({"role":"user","content":user_q})
        with st.chat_message("user"):
            st.markdown(user_q)

        # Configure the retriever with user-chosen parameters
        retriever = st.session_state.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": k, "fetch_k": fetch_k, "lambda_mult": lambda_mult}
        )
        # Create a RetrievalQA chain injecting the custom prompt
        qa = RetrievalQA.from_chain_type(
              llm=llm,
              retriever=retriever,
              chain_type="stuff",
              chain_type_kwargs={"prompt": prompt_to_use},
              return_source_documents=True
        )

        # Invoke the chain and show a spinner
        with st.spinner("ü§î Thinking‚Ä¶"):
            resp = qa.invoke({"query": user_q})
        answer = resp["result"]

        # Save and display the assistant‚Äôs answer
        st.session_state.messages.append({"role":"assistant","content":answer})
        with st.chat_message("assistant"):
            st.markdown(answer)
else:
    # Prompt to index PDFs if none are loaded
    st.info("üì• Please upload and process PDFs to start.")